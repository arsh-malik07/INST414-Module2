{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db13253-030d-4b91-8195-077fcbbc7eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas networkx matplotlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f6e1c0-7ff1-47f9-b3da-3474da8bd51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4 3.1 2.28.1 3.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, networkx as nx, requests, matplotlib\n",
    "print(pd.__version__, nx.__version__, requests.__version__, matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104b6ff2-0090-4d64-acad-5bb4c4123328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WMATA API key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rail_gtfs.zip\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "API_KEY = getpass(\"WMATA API key: \")\n",
    "\n",
    "import requests\n",
    "url = \"https://api.wmata.com/gtfs/rail-gtfs-static.zip\"\n",
    "r = requests.get(url, headers={\"api_key\": API_KEY}, timeout=60)\n",
    "r.raise_for_status()\n",
    "open(\"rail_gtfs.zip\",\"wb\").write(r.content)\n",
    "print(\"Saved rail_gtfs.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fe3455-b3bf-4531-b260-8114ef64cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in GTFS: ['agency.txt', 'calendar_dates.txt', 'feed_info.txt', 'levels.txt', 'pathways.txt', 'routes.txt', 'shapes.txt', 'stop_times.txt', 'stops.txt', 'trips.txt']\n"
     ]
    }
   ],
   "source": [
    "import zipfile, os\n",
    "\n",
    "ZIP = \"rail_gtfs.zip\" \n",
    "assert os.path.exists(ZIP), \"rail_gtfs.zip not found in this folder.\"\n",
    "\n",
    "with zipfile.ZipFile(ZIP) as z:\n",
    "    print(\"Files in GTFS:\", sorted(z.namelist())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff0ec5c-178a-4017-80f7-bb1944841c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>betweenness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L'ENFANT PLAZA</td>\n",
       "      <td>0.550974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>GALLERY PLACE METRORAIL STATION</td>\n",
       "      <td>0.399842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PENTAGON METRORAIL STATION</td>\n",
       "      <td>0.365335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ROSSLYN METRORAIL STATION</td>\n",
       "      <td>0.332617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>METRO CENTER METRORAIL STATION</td>\n",
       "      <td>0.329109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>COURT HOUSE METRORAIL STATION</td>\n",
       "      <td>0.305412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>CLARENDON METRORAIL STATION</td>\n",
       "      <td>0.292096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>VIRGINIA SQ-GMU METRORAIL STATION</td>\n",
       "      <td>0.278351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>ARCHIVES METRORAIL STATION</td>\n",
       "      <td>0.272265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>BALLSTON-MU METRORAIL STATION</td>\n",
       "      <td>0.264175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         station_name  betweenness\n",
       "11                     L'ENFANT PLAZA     0.550974\n",
       "62    GALLERY PLACE METRORAIL STATION     0.399842\n",
       "20         PENTAGON METRORAIL STATION     0.365335\n",
       "18          ROSSLYN METRORAIL STATION     0.332617\n",
       "14     METRO CENTER METRORAIL STATION     0.329109\n",
       "44      COURT HOUSE METRORAIL STATION     0.305412\n",
       "43        CLARENDON METRORAIL STATION     0.292096\n",
       "42  VIRGINIA SQ-GMU METRORAIL STATION     0.278351\n",
       "63         ARCHIVES METRORAIL STATION     0.272265\n",
       "41      BALLSTON-MU METRORAIL STATION     0.264175"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, zipfile, io, networkx as nx\n",
    "\n",
    "ZIP = \"rail_gtfs.zip\"\n",
    "\n",
    "with zipfile.ZipFile(ZIP) as z:\n",
    "    stops = pd.read_csv(io.TextIOWrapper(z.open(\"stops.txt\"), encoding=\"utf-8\"))\n",
    "    stime = pd.read_csv(io.TextIOWrapper(z.open(\"stop_times.txt\"), encoding=\"utf-8\"))[\n",
    "        [\"trip_id\",\"stop_id\",\"stop_sequence\"]\n",
    "    ]\n",
    "\n",
    "# roll platforms up to parent stations so nodes = stations (not platforms)\n",
    "stops[\"station_id\"] = stops.apply(\n",
    "    lambda r: r[\"parent_station\"] if pd.notna(r.get(\"parent_station\")) else r[\"stop_id\"], axis=1\n",
    ")\n",
    "name_by_station = (stops.dropna(subset=[\"station_id\"])\n",
    "                   .drop_duplicates(subset=[\"station_id\"])\n",
    "                   .set_index(\"station_id\")[\"stop_name\"])\n",
    "stop_to_station = stops.set_index(\"stop_id\")[\"station_id\"].to_dict()\n",
    "\n",
    "# undirected edges between consecutive stations on each trip\n",
    "pairs = []\n",
    "for _, seg in stime.sort_values([\"trip_id\",\"stop_sequence\"]).groupby(\"trip_id\"):\n",
    "    sids = [stop_to_station[s] for s in seg[\"stop_id\"] if s in stop_to_station]\n",
    "    for a, b in zip(sids, sids[1:]):\n",
    "        if a != b:\n",
    "            pairs.append(tuple(sorted((a, b))))\n",
    "\n",
    "# dedupe and build graph\n",
    "G = nx.Graph()\n",
    "for u, v in pd.Series(pairs).drop_duplicates():\n",
    "    G.add_edge(u, v)\n",
    "\n",
    "bet = nx.betweenness_centrality(G, normalized=True, weight=None)\n",
    "cent = (pd.Series(bet).rename_axis(\"station_id\").reset_index(name=\"betweenness\"))\n",
    "cent[\"station_name\"] = cent[\"station_id\"].map(name_by_station)\n",
    "cent = cent[[\"station_name\",\"betweenness\"]].dropna()\n",
    "cent.sort_values(\"betweenness\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eebe652-03e3-455a-93e2-82bd7a36aa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arsh\\AppData\\Local\\Temp\\ipykernel_67184\\2217431899.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  rid[\"date\"] = pd.to_datetime(rid[\"date\"], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>avg_weekday_entries_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addison Road</td>\n",
       "      <td>329.992469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anacostia</td>\n",
       "      <td>854.751464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Archives</td>\n",
       "      <td>1055.102929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arlington Cemetery</td>\n",
       "      <td>201.408476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ashburn</td>\n",
       "      <td>285.724686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_name  avg_weekday_entries_2024\n",
       "0        Addison Road                329.992469\n",
       "1           Anacostia                854.751464\n",
       "2            Archives               1055.102929\n",
       "3  Arlington Cemetery                201.408476\n",
       "4             Ashburn                285.724686"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "YEAR = 2024\n",
    "RID_FILE = \"ridership_export.csv\"  \n",
    "\n",
    "rid = pd.read_csv(RID_FILE, low_memory=False)\n",
    "rid.columns = rid.columns.str.lower().str.strip().str.replace(r\"\\s+\",\"_\", regex=True)\n",
    "\n",
    "# pick station column and one numeric entries measure\n",
    "station_col = \"station_name\" if \"station_name\" in rid.columns else (\"station\" if \"station\" in rid.columns else None)\n",
    "assert station_col, f\"No station column found. Columns: {rid.columns.tolist()}\"\n",
    "\n",
    "if \"entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"entries\"], errors=\"coerce\")\n",
    "elif {\"tap_entries\",\"nontapped_entries\"} <= set(rid.columns):\n",
    "    rid[\"entries_val\"] = (pd.to_numeric(rid[\"tap_entries\"], errors=\"coerce\").fillna(0) +\n",
    "                          pd.to_numeric(rid[\"nontapped_entries\"], errors=\"coerce\").fillna(0))\n",
    "else:\n",
    "    raise ValueError(\"Need 'Entries' or 'Tap Entries' + 'NonTapped Entries' in your export.\")\n",
    "\n",
    "# belt-and-suspenders filters if present\n",
    "if \"date\" in rid.columns:\n",
    "    rid[\"date\"] = pd.to_datetime(rid[\"date\"], errors=\"coerce\")\n",
    "    rid = rid[rid[\"date\"].between(f\"{YEAR}-01-01\", f\"{YEAR}-12-31\")]\n",
    "if \"day_of_week\" in rid.columns:\n",
    "    wk = {\"mon\",\"tue\",\"wed\",\"thu\",\"fri\"}\n",
    "    rid = rid[rid[\"day_of_week\"].astype(str).str.lower().str[:3].isin(wk)]\n",
    "\n",
    "# collapse time-of-day to all-day totals, then average across weekdays\n",
    "group = [station_col]\n",
    "if \"date\" in rid.columns: group += [\"date\"]\n",
    "if \"time_period\" in rid.columns: group += [\"time_period\"]\n",
    "\n",
    "day_sum = rid.groupby(group, as_index=False)[\"entries_val\"].sum()\n",
    "by_station = (day_sum.groupby(station_col, as_index=False)[\"entries_val\"]\n",
    "              .mean().rename(columns={\"entries_val\": \"avg_weekday_entries_2024\"}))\n",
    "\n",
    "# tidy name column\n",
    "by_station = by_station.rename(columns={station_col: \"station_name\"})\n",
    "by_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3334c37-5b4e-4a06-9d26-e85fda82eecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>avg_weekday_entries_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [station_name, avg_weekday_entries_2024]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> wmata_ridership_by_station_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# === Build \"2024 average weekday entries by station\" from ridership_export.csv ===\n",
    "import pandas as pd\n",
    "YEAR = 2024\n",
    "RID_FILE = \"ridership_export.csv\"   # leave as-is if your file is named this\n",
    "\n",
    "# Robust read (Tableau exports can be utf-16)\n",
    "def read_tableau_csv(p):\n",
    "    try:\n",
    "        return pd.read_csv(p, low_memory=False)\n",
    "    except UnicodeError:\n",
    "        return pd.read_csv(p, encoding=\"utf-16\", engine=\"python\", low_memory=False)\n",
    "\n",
    "rid = read_tableau_csv(RID_FILE)\n",
    "rid.columns = rid.columns.str.lower().str.strip().str.replace(r\"\\s+\",\"_\", regex=True)\n",
    "\n",
    "# pick columns present in your file\n",
    "station_col = \"station_name\" if \"station_name\" in rid.columns else (\"station\" if \"station\" in rid.columns else None)\n",
    "assert station_col, f\"No 'Station Name' column found. Got: {rid.columns.tolist()}\"\n",
    "\n",
    "# build a single numeric entries value\n",
    "if \"entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"entries\"], errors=\"coerce\")\n",
    "elif {\"tap_entries\",\"nontapped_entries\"} <= set(rid.columns):\n",
    "    rid[\"entries_val\"] = (pd.to_numeric(rid[\"tap_entries\"], errors=\"coerce\").fillna(0) +\n",
    "                          pd.to_numeric(rid[\"nontapped_entries\"], errors=\"coerce\").fillna(0))\n",
    "elif \"avg_daily_entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"avg_daily_entries\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise ValueError(\"Need 'Entries' or 'Tap Entries' + 'NonTapped Entries' (or 'Avg Daily Entries').\")\n",
    "\n",
    "# belt-and-suspenders filters if present\n",
    "if \"date\" in rid.columns:\n",
    "    # set the exact format if you know it; else let pandas infer:\n",
    "    try:\n",
    "        rid[\"date\"] = pd.to_datetime(rid[\"date\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        rid[\"date\"] = pd.to_datetime(rid[\"date\"], errors=\"coerce\")\n",
    "    rid = rid[rid[\"date\"].between(f\"{YEAR}-01-01\", f\"{YEAR}-12-31\")]\n",
    "\n",
    "if \"day_of_week\" in rid.columns:\n",
    "    wk = {\"mon\",\"tue\",\"wed\",\"thu\",\"fri\"}\n",
    "    rid = rid[rid[\"day_of_week\"].astype(str).str.lower().str[:3].isin(wk)]\n",
    "\n",
    "# collapse any time-of-day to all-day totals, then average across days\n",
    "group = [station_col]\n",
    "if \"date\" in rid.columns: group.append(\"date\")\n",
    "if \"time_period\" in rid.columns: group.append(\"time_period\")\n",
    "\n",
    "day_sum = rid.groupby(group, as_index=False)[\"entries_val\"].sum()\n",
    "by_station = (day_sum.groupby(station_col, as_index=False)[\"entries_val\"]\n",
    "              .mean()\n",
    "              .rename(columns={station_col: \"station_name\", \"entries_val\": \"avg_weekday_entries_2024\"}))\n",
    "\n",
    "by_station = by_station.sort_values(\"avg_weekday_entries_2024\", ascending=False)\n",
    "print(\"Stations:\", len(by_station))\n",
    "display(by_station.head(10))\n",
    "\n",
    "by_station.to_csv(\"wmata_ridership_by_station_2024.csv\", index=False)\n",
    "print(\"Saved -> wmata_ridership_by_station_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6442a865-477b-4abb-89a2-4582e59276bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected non-empty vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_weekday_entries_2024\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m eps)\n\u001b[0;32m     17\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetweenness\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m---> 18\u001b[0m b0, b1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolyfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m-\u001b[39m (b0 \u001b[38;5;241m+\u001b[39m b1\u001b[38;5;241m*\u001b[39mX)\n\u001b[0;32m     21\u001b[0m top_hidden  \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)[\n\u001b[0;32m     22\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstation_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetweenness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_weekday_entries_2024\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     23\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda364\\Lib\\site-packages\\numpy\\lib\\polynomial.py:639\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 1D vector for x\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected non-empty vector for x\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 1D or 2D array for y\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected non-empty vector for x"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "cent = pd.read_csv(\"wmata_betweenness.csv\")   # columns: station_name, betweenness\n",
    "rid  = pd.read_csv(\"wmata_ridership_by_station_2024.csv\")\n",
    "\n",
    "# light name normalization\n",
    "def norm(s):\n",
    "    return (str(s).upper().replace(\"’\",\"'\").strip())\n",
    "cent[\"jn\"] = cent[\"station_name\"].map(norm)\n",
    "rid[\"jn\"]  = rid[\"station_name\"].map(norm)\n",
    "\n",
    "df = cent.merge(rid, on=\"jn\", suffixes=(\"_cent\",\"_rid\"))\n",
    "df[\"station_name\"] = df[\"station_name_cent\"]\n",
    "\n",
    "# log–log residuals\n",
    "eps = 1e-9\n",
    "X = np.log(df[\"avg_weekday_entries_2024\"] + eps)\n",
    "Y = np.log(df[\"betweenness\"] + eps)\n",
    "b0, b1 = np.polyfit(X, Y, 1)\n",
    "df[\"residual\"] = Y - (b0 + b1*X)\n",
    "\n",
    "top_hidden  = df.sort_values(\"residual\", ascending=False).head(10)[\n",
    "    [\"station_name\",\"betweenness\",\"avg_weekday_entries_2024\",\"residual\"]\n",
    "]\n",
    "top_crowded = df.sort_values(\"residual\", ascending=True).head(10)[\n",
    "    [\"station_name\",\"betweenness\",\"avg_weekday_entries_2024\",\"residual\"]\n",
    "]\n",
    "\n",
    "display(top_hidden.head(5))\n",
    "display(top_crowded.head(5))\n",
    "top_hidden.to_csv(\"top_hidden_chokepoints.csv\", index=False)\n",
    "top_crowded.to_csv(\"top_crowded_not_central.csv\", index=False)\n",
    "\n",
    "# quick figure\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X, Y, alpha=0.5)\n",
    "xg = np.linspace(X.min(), X.max(), 200)\n",
    "plt.plot(xg, b0 + b1*xg)\n",
    "plt.xlabel(\"log(avg weekday entries 2024)\")\n",
    "plt.ylabel(\"log(betweenness)\")\n",
    "plt.title(\"WMATA: centrality vs ridership (log–log)\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57c0498-2119-48d9-bdf0-580cb4858fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> wmata_betweenness.csv\n",
      "Stations in graph: 98 Edges: 103\n"
     ]
    }
   ],
   "source": [
    "# Compute station betweenness from WMATA GTFS and save to wmata_betweenness.csv\n",
    "import os, io, zipfile, pandas as pd, networkx as nx\n",
    "\n",
    "ZIP = \"rail_gtfs.zip\"  # change if yours has a different name\n",
    "assert os.path.exists(ZIP), f\"{ZIP} not found in {os.getcwd()}\"\n",
    "\n",
    "with zipfile.ZipFile(ZIP) as z:\n",
    "    # read GTFS core files\n",
    "    stops = pd.read_csv(io.TextIOWrapper(z.open(\"stops.txt\"), encoding=\"utf-8\"))\n",
    "    stime = pd.read_csv(io.TextIOWrapper(z.open(\"stop_times.txt\"), encoding=\"utf-8\"))[\n",
    "        [\"trip_id\",\"stop_id\",\"stop_sequence\"]\n",
    "    ]\n",
    "\n",
    "# ---- roll platforms up so nodes = stations (not platforms) ----\n",
    "# If parent_station exists, use it; otherwise, treat stop_id as its own station\n",
    "if \"parent_station\" in stops.columns:\n",
    "    stops[\"station_id\"] = stops.apply(\n",
    "        lambda r: r[\"parent_station\"] if pd.notna(r.get(\"parent_station\")) and str(r[\"parent_station\"]).strip() != \"\" \n",
    "        else r[\"stop_id\"], axis=1\n",
    "    )\n",
    "else:\n",
    "    stops[\"station_id\"] = stops[\"stop_id\"]\n",
    "\n",
    "# Pick one name per station_id (prefer rows marked as stations if present)\n",
    "if \"location_type\" in stops.columns:\n",
    "    # location_type==1 are stations; 0 are platforms\n",
    "    name_df = (stops.sort_values([\"station_id\",\"location_type\"], ascending=[True, False])\n",
    "                     .drop_duplicates(subset=[\"station_id\"]))\n",
    "else:\n",
    "    name_df = stops.drop_duplicates(subset=[\"station_id\"])\n",
    "name_by_station = name_df.set_index(\"station_id\")[\"stop_name\"]\n",
    "\n",
    "# Map stop_id -> station_id for edge building\n",
    "stop_to_station = stops.set_index(\"stop_id\")[\"station_id\"].to_dict()\n",
    "\n",
    "# ---- build undirected edges between consecutive stations on each trip ----\n",
    "pairs = []\n",
    "for _, seg in stime.sort_values([\"trip_id\",\"stop_sequence\"]).groupby(\"trip_id\"):\n",
    "    sids = [stop_to_station[s] for s in seg[\"stop_id\"] if s in stop_to_station]\n",
    "    for a, b in zip(sids, sids[1:]):\n",
    "        if a != b:\n",
    "            pairs.append(tuple(sorted((a, b))))\n",
    "\n",
    "# dedupe edges\n",
    "edges = pd.Series(pairs).drop_duplicates().tolist()\n",
    "\n",
    "# ---- graph + betweenness ----\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(name_by_station.index)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "bet = nx.betweenness_centrality(G, normalized=True, weight=None)\n",
    "\n",
    "cent = (pd.Series(bet, name=\"betweenness\")\n",
    "          .rename_axis(\"station_id\")\n",
    "          .reset_index())\n",
    "cent[\"station_name\"] = cent[\"station_id\"].map(name_by_station)\n",
    "cent = cent[[\"station_name\",\"betweenness\"]].dropna()\n",
    "\n",
    "cent.sort_values(\"betweenness\", ascending=False).head(10)\n",
    "cent.to_csv(\"wmata_betweenness.csv\", index=False)\n",
    "print(\"Saved -> wmata_betweenness.csv\")\n",
    "print(\"Stations in graph:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aeb43ad-6ece-4d35-b037-296a919e21ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTFS stations: 98 Ridership stations: 0\n",
      "\n",
      "Sample GTFS names: ['NOMA-GALLAUDET U', 'DUNN LORING', 'DUPONT CIRCLE', 'SHAW-HOWARD U', 'SOUTHERN AVE', 'BALLSTON-MU', 'JUDICIARY SQUARE', 'FRANCONIA SPRINGFIELD']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGTFS stations:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cent\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRidership stations:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rid\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample GTFS names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstation_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample ridership names:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mrid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstation_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32m~\\anaconda364\\Lib\\site-packages\\pandas\\core\\generic.py:6029\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[0;32m   6026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6027\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[1;32m-> 6029\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6030\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   6032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[1;32m~\\anaconda364\\Lib\\site-packages\\pandas\\core\\sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[1;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    154\u001b[0m )\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:945\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cent = pd.read_csv(\"wmata_betweenness.csv\")              # from GTFS step\n",
    "rid  = pd.read_csv(\"wmata_ridership_by_station_2024.csv\")# from your export aggregation\n",
    "\n",
    "print(\"GTFS stations:\", cent.shape[0], \"Ridership stations:\", rid.shape[0])\n",
    "\n",
    "print(\"\\nSample GTFS names:\", cent['station_name'].dropna().sample(8, random_state=0).tolist())\n",
    "print(\"Sample ridership names:\", rid['station_name'].dropna().sample(8, random_state=1).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a59d6922-85cf-4361-9089-b9aa5418fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw shape: (755547, 13)\n",
      "raw columns: ['Year of Date', 'Date', 'Day of Week', 'Holiday', 'Service Type', 'Station Name', 'Time Period', 'Year', 'Avg Daily Tapped Entries', 'Entries', 'NonTapped Entries', 'SUM([NonTapped Entries])/COUNTD([Date])', 'Tap Entries']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year of Date</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Service Type</th>\n",
       "      <th>Station Name</th>\n",
       "      <th>Time Period</th>\n",
       "      <th>Year</th>\n",
       "      <th>Avg Daily Tapped Entries</th>\n",
       "      <th>Entries</th>\n",
       "      <th>NonTapped Entries</th>\n",
       "      <th>SUM([NonTapped Entries])/COUNTD([Date])</th>\n",
       "      <th>Tap Entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>2/17/2024 12:00:00 AM</td>\n",
       "      <td>Sat</td>\n",
       "      <td>No</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Tysons</td>\n",
       "      <td>AM Peak (Open-9:30am)</td>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>5/25/2024 12:00:00 AM</td>\n",
       "      <td>Sat</td>\n",
       "      <td>No</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Court House</td>\n",
       "      <td>AM Peak (Open-9:30am)</td>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>6/29/2024 12:00:00 AM</td>\n",
       "      <td>Sat</td>\n",
       "      <td>No</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Arlington Cemetery</td>\n",
       "      <td>AM Peak (Open-9:30am)</td>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>1/27/2024 12:00:00 AM</td>\n",
       "      <td>Sat</td>\n",
       "      <td>No</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Van Ness-UDC</td>\n",
       "      <td>AM Peak (Open-9:30am)</td>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>8/31/2024 12:00:00 AM</td>\n",
       "      <td>Sat</td>\n",
       "      <td>No</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Pentagon</td>\n",
       "      <td>AM Peak (Open-9:30am)</td>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year of Date                   Date Day of Week Holiday Service Type  \\\n",
       "0          2024  2/17/2024 12:00:00 AM         Sat      No     Saturday   \n",
       "1          2024  5/25/2024 12:00:00 AM         Sat      No     Saturday   \n",
       "2          2024  6/29/2024 12:00:00 AM         Sat      No     Saturday   \n",
       "3          2024  1/27/2024 12:00:00 AM         Sat      No     Saturday   \n",
       "4          2024  8/31/2024 12:00:00 AM         Sat      No     Saturday   \n",
       "\n",
       "         Station Name            Time Period  Year  Avg Daily Tapped Entries  \\\n",
       "0              Tysons  AM Peak (Open-9:30am)  2024                         0   \n",
       "1         Court House  AM Peak (Open-9:30am)  2024                         0   \n",
       "2  Arlington Cemetery  AM Peak (Open-9:30am)  2024                         0   \n",
       "3        Van Ness-UDC  AM Peak (Open-9:30am)  2024                         0   \n",
       "4            Pentagon  AM Peak (Open-9:30am)  2024                         0   \n",
       "\n",
       "   Entries  NonTapped Entries  SUM([NonTapped Entries])/COUNTD([Date])  \\\n",
       "0        2                  0                                        0   \n",
       "1        0                  0                                        0   \n",
       "2        0                  0                                        0   \n",
       "3        0                  0                                        0   \n",
       "4        0                  0                                        0   \n",
       "\n",
       "   Tap Entries  \n",
       "0            2  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# robust reader (handles Tableau CSV encodings)\n",
    "def read_tableau_csv(p):\n",
    "    try:\n",
    "        return pd.read_csv(p, low_memory=False)\n",
    "    except UnicodeError:\n",
    "        return pd.read_csv(p, encoding=\"utf-16\", engine=\"python\", low_memory=False)\n",
    "\n",
    "raw = read_tableau_csv(\"ridership_export.csv\")\n",
    "print(\"raw shape:\", raw.shape)\n",
    "print(\"raw columns:\", list(raw.columns))\n",
    "display(raw.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db371e1e-d58c-4b10-8c7c-830767f295cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized columns: ['year_of_date', 'date', 'day_of_week', 'holiday', 'service_type', 'station_name', 'time_period', 'year', 'avg_daily_tapped_entries', 'entries', 'nontapped_entries', 'sum([nontapped_entries])/countd([date])', 'tap_entries']\n",
      "has station_name? True\n",
      "numeric-ish columns: ['avg_daily_tapped_entries', 'entries', 'nontapped_entries', 'sum([nontapped_entries])/countd([date])', 'tap_entries']\n"
     ]
    }
   ],
   "source": [
    "rid = raw.copy()\n",
    "rid.columns = rid.columns.str.lower().str.strip().str.replace(r\"\\s+\",\"_\", regex=True)\n",
    "\n",
    "print(\"normalized columns:\", list(rid.columns))\n",
    "print(\"has station_name?\", \"station_name\" in rid.columns or \"station\" in rid.columns)\n",
    "\n",
    "# What numeric measures do we have?\n",
    "cands = [c for c in rid.columns if any(k in c for k in\n",
    "    [\"entries\",\"tap\",\"nontapped\",\"non_tapped\",\"avg_daily\"])]\n",
    "print(\"numeric-ish columns:\", cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dd05340-e418-4a30-bc2f-b06d460d1d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date min/max: NaT NaT\n",
      "⚠️ No 2024 dates detected; skipping the 2024 filter so we don't drop everything.\n",
      "stations in by_station: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>avg_weekday_entries_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [station_name, avg_weekday_entries_2024]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pick station column\n",
    "station_col = \"station_name\" if \"station_name\" in rid.columns else (\"station\" if \"station\" in rid.columns else None)\n",
    "assert station_col, f\"No station column found. Got: {rid.columns.tolist()}\"\n",
    "\n",
    "# build a single numeric entries column\n",
    "if \"entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"entries\"], errors=\"coerce\")\n",
    "elif {\"tap_entries\",\"nontapped_entries\"} <= set(rid.columns):\n",
    "    rid[\"entries_val\"] = (pd.to_numeric(rid[\"tap_entries\"], errors=\"coerce\").fillna(0) +\n",
    "                          pd.to_numeric(rid[\"nontapped_entries\"], errors=\"coerce\").fillna(0))\n",
    "elif \"avg_daily_entries\" in rid.columns:\n",
    "    # use only if it’s clearly non-zero\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"avg_daily_entries\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise ValueError(\"Need 'Entries' or 'Tap Entries' + 'NonTapped Entries' (or non-zero 'Avg Daily Entries').\")\n",
    "\n",
    "# optional: filter to 2024 only *IF* a date column exists\n",
    "if \"date\" in rid.columns:\n",
    "    # try common WMATA formats first; fall back to infer\n",
    "    try:\n",
    "        rid[\"date\"] = pd.to_datetime(rid[\"date\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        rid[\"date\"] = pd.to_datetime(rid[\"date\"], errors=\"coerce\")\n",
    "    print(\"date min/max:\", rid[\"date\"].min(), rid[\"date\"].max())\n",
    "    # keep 2024 only — but only if we actually have 2024 rows\n",
    "    has_2024 = (rid[\"date\"].dt.year == 2024).any()\n",
    "    if has_2024:\n",
    "        rid = rid[(rid[\"date\"] >= \"2024-01-01\") & (rid[\"date\"] <= \"2024-12-31\")]\n",
    "    else:\n",
    "        print(\"⚠️ No 2024 dates detected; skipping the 2024 filter so we don't drop everything.\")\n",
    "\n",
    "# optional: weekdays only if a day-of-week column exists\n",
    "for dow in [\"day_of_week\",\"weekday\",\"day_type\"]:\n",
    "    if dow in rid.columns:\n",
    "        wk = {\"mon\",\"tue\",\"wed\",\"thu\",\"fri\"}\n",
    "        rid = rid[rid[dow].astype(str).str.lower().str[:3].isin(wk)]\n",
    "        break\n",
    "\n",
    "# collapse time-of-day to daily totals if a time_period column exists\n",
    "group = [station_col]\n",
    "if \"date\" in rid.columns: group.append(\"date\")\n",
    "if \"time_period\" in rid.columns: group.append(\"time_period\")\n",
    "\n",
    "day_sum = rid.groupby(group, as_index=False)[\"entries_val\"].sum()\n",
    "\n",
    "# now average to station level\n",
    "by_station = (day_sum.groupby(station_col, as_index=False)[\"entries_val\"]\n",
    "              .mean()\n",
    "              .rename(columns={station_col:\"station_name\", \"entries_val\":\"avg_weekday_entries_2024\"}))\n",
    "\n",
    "print(\"stations in by_station:\", len(by_station))\n",
    "display(by_station.head(10))\n",
    "\n",
    "# save for the join step\n",
    "by_station.to_csv(\"wmata_ridership_by_station_2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6d8911-c6ba-493a-b5fa-ad1ca9a15441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years in file:\n",
      " Series([], Name: count, dtype: int64)\n",
      "⚠️ No 2024 rows detected. Using latest available year: None\n",
      "Stations aggregated: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>avg_weekday_entries_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [station_name, avg_weekday_entries_2024]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> wmata_ridership_by_station_2024.csv (if not truly 2024, this still holds your chosen year).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "RID_FILE = \"ridership_export.csv\"  # change if your filename is different\n",
    "\n",
    "def read_tableau_csv(p):\n",
    "    try:\n",
    "        return pd.read_csv(p, low_memory=False)\n",
    "    except UnicodeError:\n",
    "        return pd.read_csv(p, encoding=\"utf-16\", engine=\"python\", low_memory=False)\n",
    "\n",
    "rid = read_tableau_csv(RID_FILE)\n",
    "rid.columns = rid.columns.str.lower().str.strip().str.replace(r\"\\s+\",\"_\", regex=True)\n",
    "\n",
    "# --- pick station column ---\n",
    "station_col = \"station_name\" if \"station_name\" in rid.columns else (\"station\" if \"station\" in rid.columns else None)\n",
    "assert station_col, f\"No station column found. Got: {rid.columns.tolist()}\"\n",
    "\n",
    "# --- build numeric entries value ---\n",
    "if \"entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"entries\"], errors=\"coerce\")\n",
    "elif {\"tap_entries\",\"nontapped_entries\"} <= set(rid.columns):\n",
    "    rid[\"entries_val\"] = (pd.to_numeric(rid[\"tap_entries\"], errors=\"coerce\").fillna(0) +\n",
    "                          pd.to_numeric(rid[\"nontapped_entries\"], errors=\"coerce\").fillna(0))\n",
    "elif \"avg_daily_entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"avg_daily_entries\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise ValueError(\"Need 'Entries' or 'Tap Entries' + 'NonTapped Entries' (or non-zero 'Avg Daily Entries').\")\n",
    "\n",
    "# --- parse dates if present; choose best year automatically ---\n",
    "date_col = \"date\" if \"date\" in rid.columns else None\n",
    "chosen_year = None\n",
    "\n",
    "if date_col:\n",
    "    # Try common WMATA formats first; fall back to infer\n",
    "    try:\n",
    "        rid[date_col] = pd.to_datetime(rid[date_col], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    except Exception:\n",
    "        rid[date_col] = pd.to_datetime(rid[date_col], errors=\"coerce\")\n",
    "\n",
    "    years = rid[date_col].dt.year.dropna().astype(int)\n",
    "    year_counts = years.value_counts().sort_index()\n",
    "    print(\"Years in file:\\n\", year_counts)\n",
    "\n",
    "    if (years == 2024).any():\n",
    "        chosen_year = 2024\n",
    "        print(\"Using preferred year 2024.\")\n",
    "    else:\n",
    "        chosen_year = int(years.max()) if len(years) else None\n",
    "        print(f\"⚠️ No 2024 rows detected. Using latest available year: {chosen_year}\")\n",
    "\n",
    "    if chosen_year is not None:\n",
    "        rid = rid[rid[date_col].dt.year == chosen_year]\n",
    "\n",
    "    # Weekdays: use provided day_of_week if present, else derive from date\n",
    "    if \"day_of_week\" in rid.columns:\n",
    "        wk = {\"mon\",\"tue\",\"wed\",\"thu\",\"fri\"}\n",
    "        rid = rid[rid[\"day_of_week\"].astype(str).str.lower().str[:3].isin(wk)]\n",
    "    else:\n",
    "        rid = rid[rid[date_col].dt.dayofweek <= 4]  # 0=Mon ... 4=Fri\n",
    "\n",
    "# --- collapse time-of-day to daily totals, then average by station ---\n",
    "group = [station_col]\n",
    "if date_col: group.append(date_col)\n",
    "if \"time_period\" in rid.columns and date_col:\n",
    "    day_sum = rid.groupby([station_col, date_col], as_index=False)[\"entries_val\"].sum()\n",
    "elif date_col:\n",
    "    day_sum = rid[[station_col, date_col, \"entries_val\"]].copy()\n",
    "else:\n",
    "    # No dates: assume export already reflects your intended period; sum across time_period if present\n",
    "    if \"time_period\" in rid.columns:\n",
    "        day_sum = rid.groupby(station_col, as_index=False)[\"entries_val\"].sum().assign(dummy_date=pd.NaT)\n",
    "        date_col = \"dummy_date\"\n",
    "    else:\n",
    "        day_sum = rid.groupby(station_col, as_index=False)[\"entries_val\"].mean().assign(dummy_date=pd.NaT)\n",
    "        date_col = \"dummy_date\"\n",
    "\n",
    "by_station = (day_sum.groupby(station_col, as_index=False)[\"entries_val\"]\n",
    "              .mean()\n",
    "              .rename(columns={station_col: \"station_name\", \"entries_val\": \"avg_weekday_entries_2024\"}))\n",
    "\n",
    "print(\"Stations aggregated:\", len(by_station))\n",
    "display(by_station.head(10))\n",
    "\n",
    "# Save under the expected name (even if the year used wasn't 2024)\n",
    "by_station.to_csv(\"wmata_ridership_by_station_2024.csv\", index=False)\n",
    "print(\"Saved -> wmata_ridership_by_station_2024.csv (if not truly 2024, this still holds your chosen year).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73b0479b-d3a5-42a8-b985-08106e65f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Date column not parseable in a consistent way; ignoring dates and proceeding.\n",
      "✅ Stations aggregated: 98\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>avg_weekday_entries_2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Addison Road</td>\n",
       "      <td>465856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anacostia</td>\n",
       "      <td>1206700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Archives</td>\n",
       "      <td>1574083.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arlington Cemetery</td>\n",
       "      <td>346356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ashburn</td>\n",
       "      <td>431765.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ballston-MU</td>\n",
       "      <td>1807365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Benning Road</td>\n",
       "      <td>597249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bethesda</td>\n",
       "      <td>1535317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Braddock Road</td>\n",
       "      <td>715322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Branch Ave</td>\n",
       "      <td>775906.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_name  avg_weekday_entries_2024\n",
       "0        Addison Road                  465856.0\n",
       "1           Anacostia                 1206700.0\n",
       "2            Archives                 1574083.0\n",
       "3  Arlington Cemetery                  346356.0\n",
       "4             Ashburn                  431765.0\n",
       "5         Ballston-MU                 1807365.0\n",
       "6        Benning Road                  597249.0\n",
       "7            Bethesda                 1535317.0\n",
       "8       Braddock Road                  715322.0\n",
       "9          Branch Ave                  775906.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> wmata_ridership_by_station_2024.csv (label says 2024 even if dates were missing; it contains the period in your export).\n"
     ]
    }
   ],
   "source": [
    "# === Bulletproof ridership loader -> station averages ===\n",
    "import pandas as pd, numpy as np, re\n",
    "\n",
    "RID_FILE = \"ridership_export.csv\"  # change if your filename differs\n",
    "\n",
    "def read_tableau_csv(p):\n",
    "    # Try common encodings Tableau uses\n",
    "    for enc in [None, \"utf-16\", \"utf-8-sig\"]:\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=enc, engine=\"python\", low_memory=False) if enc else pd.read_csv(p, low_memory=False)\n",
    "        except Exception as e:\n",
    "            last_e = e\n",
    "    raise last_e\n",
    "\n",
    "rid = read_tableau_csv(RID_FILE)\n",
    "rid.columns = rid.columns.str.lower().str.strip().str.replace(r\"\\s+\",\"_\", regex=True)\n",
    "\n",
    "# 1) Station column\n",
    "station_col = \"station_name\" if \"station_name\" in rid.columns else (\"station\" if \"station\" in rid.columns else None)\n",
    "assert station_col, f\"No station column found. Columns: {list(rid.columns)}\"\n",
    "\n",
    "# 2) Numeric entries column\n",
    "if \"entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"entries\"], errors=\"coerce\")\n",
    "elif {\"tap_entries\",\"nontapped_entries\"} <= set(rid.columns):\n",
    "    rid[\"entries_val\"] = (pd.to_numeric(rid[\"tap_entries\"], errors=\"coerce\").fillna(0) +\n",
    "                          pd.to_numeric(rid[\"nontapped_entries\"], errors=\"coerce\").fillna(0))\n",
    "elif \"avg_daily_entries\" in rid.columns:\n",
    "    rid[\"entries_val\"] = pd.to_numeric(rid[\"avg_daily_entries\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise ValueError(\"Need 'Entries' or 'Tap Entries' + 'NonTapped Entries' (or non-zero 'Avg Daily Entries').\")\n",
    "\n",
    "# 3) Dates: try to use them; otherwise ignore gracefully\n",
    "date_col = \"date\" if \"date\" in rid.columns else None\n",
    "used_year = None\n",
    "if date_col:\n",
    "    # Try several formats, then infer\n",
    "    parsed = None\n",
    "    for fmt in [\"%m/%d/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y %I:%M %p\", \"%Y-%m-%d %H:%M:%S\"]:\n",
    "        try:\n",
    "            parsed = pd.to_datetime(rid[date_col], format=fmt, errors=\"coerce\")\n",
    "            if parsed.notna().mean() > 0.5:\n",
    "                break\n",
    "        except Exception:\n",
    "            parsed = None\n",
    "    if parsed is None or parsed.notna().mean() <= 0.2:\n",
    "        print(\"⚠️ Date column not parseable in a consistent way; ignoring dates and proceeding.\")\n",
    "        date_col = None\n",
    "    else:\n",
    "        rid[date_col] = parsed\n",
    "        counts = rid[date_col].dt.year.value_counts().sort_index()\n",
    "        print(\"Years present:\", counts.to_dict())\n",
    "        if (rid[date_col].dt.year == 2024).any():\n",
    "            used_year = 2024\n",
    "        else:\n",
    "            used_year = int(counts.index.max())\n",
    "            print(f\"⚠️ No 2024 rows; using latest available year: {used_year}\")\n",
    "        rid = rid[rid[date_col].dt.year == used_year]\n",
    "        # Weekdays: use provided day_of_week if present, else derive\n",
    "        if \"day_of_week\" in rid.columns:\n",
    "            wk = {\"mon\",\"tue\",\"wed\",\"thu\",\"fri\"}\n",
    "            rid = rid[rid[\"day_of_week\"].astype(str).str.lower().str[:3].isin(wk)]\n",
    "        else:\n",
    "            rid = rid[rid[date_col].dt.dayofweek <= 4]\n",
    "\n",
    "# 4) Collapse time-of-day if needed, then average by station\n",
    "group = [station_col]\n",
    "if date_col: group.append(date_col)\n",
    "if \"time_period\" in rid.columns and date_col:\n",
    "    day_sum = rid.groupby([station_col, date_col], as_index=False)[\"entries_val\"].sum()\n",
    "elif date_col:\n",
    "    day_sum = rid[[station_col, date_col, \"entries_val\"]].copy()\n",
    "else:\n",
    "    # No usable dates: aggregate to station totals over whatever period the export represents\n",
    "    if \"time_period\" in rid.columns:\n",
    "        day_sum = rid.groupby(station_col, as_index=False)[\"entries_val\"].sum().assign(dummy_date=pd.NaT)\n",
    "        date_col = \"dummy_date\"\n",
    "    else:\n",
    "        day_sum = rid.groupby(station_col, as_index=False)[\"entries_val\"].mean().assign(dummy_date=pd.NaT)\n",
    "        date_col = \"dummy_date\"\n",
    "\n",
    "by_station = (day_sum.groupby(station_col, as_index=False)[\"entries_val\"]\n",
    "              .mean()\n",
    "              .rename(columns={station_col: \"station_name\", \"entries_val\": \"avg_weekday_entries_2024\"}))\n",
    "\n",
    "print(\"✅ Stations aggregated:\", len(by_station))\n",
    "display(by_station.head(10))\n",
    "\n",
    "by_station.to_csv(\"wmata_ridership_by_station_2024.csv\", index=False)\n",
    "print(\"Saved -> wmata_ridership_by_station_2024.csv\",\n",
    "      \"(label says 2024 even if dates were missing; it contains the period in your export).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c90e4dc-e7be-4036-98e2-17f493f8369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: top_hidden_chokepoints.csv, top_crowded_not_central.csv, figs/centrality_vs_ridership.png, figs/punch_above_weight.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>avg_weekday_entries_2024</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>METRO CENTER</td>\n",
       "      <td>0.329109</td>\n",
       "      <td>15767.152672</td>\n",
       "      <td>73.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UNION STATION</td>\n",
       "      <td>0.137887</td>\n",
       "      <td>16750.645038</td>\n",
       "      <td>72.732414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>L'ENFANT PLAZA</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>12083.973282</td>\n",
       "      <td>71.587076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DUPONT CIRCLE</td>\n",
       "      <td>0.219072</td>\n",
       "      <td>13117.790076</td>\n",
       "      <td>71.300927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FARRAGUT NORTH</td>\n",
       "      <td>0.234536</td>\n",
       "      <td>11654.278626</td>\n",
       "      <td>70.452422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      station_name  betweenness  avg_weekday_entries_2024   residual\n",
       "0     METRO CENTER     0.329109              15767.152672  73.133477\n",
       "15   UNION STATION     0.137887              16750.645038  72.732414\n",
       "34  L'ENFANT PLAZA     0.550974              12083.973282  71.587076\n",
       "2    DUPONT CIRCLE     0.219072              13117.790076  71.300927\n",
       "1   FARRAGUT NORTH     0.234536              11654.278626  70.452422"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Inputs created earlier\n",
    "cent = pd.read_csv(\"wmata_betweenness.csv\")                # station_name, betweenness\n",
    "rid  = pd.read_csv(\"wmata_ridership_by_station_2024_daily.csv\")  # station_name, avg_weekday_entries_2024\n",
    "\n",
    "# Light name normalization so sources match\n",
    "RENAME = {\n",
    "    \"WHITE FLINT\": \"NORTH BETHESDA\",\n",
    "    \"PRINCE GEORGE'S PLAZA\": \"HYATTSVILLE CROSSING\",\n",
    "    \"PRINCE GEORGES PLAZA\": \"HYATTSVILLE CROSSING\",\n",
    "    \"KING ST-OLD TOWN\": \"KING STREET-OLD TOWN\",\n",
    "    \"U STREET/AFRICAN-AMER CIVIL WAR MEMORIAL/CARDOZO\": \"U STREET\",\n",
    "    \"NOMA-GALLAUDET U\": \"NOMA – GALLAUDET U\",   # add/adjust if your ridership has typographic variants\n",
    "}\n",
    "def norm(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.upper().strip().replace(\"’\",\"'\").replace(\"&\",\"AND\").replace(\"/\",\" \").replace(\"–\",\"-\")\n",
    "    s = re.sub(r\"[^A-Z0-9\\- ]\",\" \", s)\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
    "    return RENAME.get(s, s)\n",
    "\n",
    "cent[\"jn\"] = cent[\"station_name\"].map(norm)\n",
    "rid[\"jn\"]  = rid[\"station_name\"].map(norm)\n",
    "\n",
    "df = cent.merge(rid, on=\"jn\", how=\"inner\", suffixes=(\"_cent\",\"_rid\")).copy()\n",
    "df[\"station_name\"] = df[\"station_name_cent\"]\n",
    "\n",
    "# Drop zeros (log-space)\n",
    "eps = 1e-9\n",
    "df = df[(df[\"betweenness\"] > 0) & (df[\"avg_weekday_entries_2024\"] > 0)]\n",
    "\n",
    "# Fit log–log trend: betweenness ~ f(entries)\n",
    "X = np.log(df[\"avg_weekday_entries_2024\"] + eps)\n",
    "Y = np.log(df[\"betweenness\"] + eps)\n",
    "b0, b1 = np.polyfit(X, Y, 1)\n",
    "df[\"residual\"] = Y - (b0 + b1*X)  # + = more central than entries predict\n",
    "\n",
    "# Top lists you’ll report\n",
    "top_hidden  = df.sort_values(\"residual\", ascending=False).head(10)[\n",
    "    [\"station_name\",\"betweenness\",\"avg_weekday_entries_2024\",\"residual\"]\n",
    "]\n",
    "top_crowded = df.sort_values(\"residual\", ascending=True).head(10)[\n",
    "    [\"station_name\",\"betweenness\",\"avg_weekday_entries_2024\",\"residual\"]\n",
    "]\n",
    "\n",
    "Path(\"figs\").mkdir(exist_ok=True)\n",
    "top_hidden.to_csv(\"top_hidden_chokepoints.csv\", index=False)\n",
    "top_crowded.to_csv(\"top_crowded_not_central.csv\", index=False)\n",
    "\n",
    "# Scatter + fit line\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X, Y, alpha=0.5)\n",
    "xg = np.linspace(X.min(), X.max(), 200); plt.plot(xg, b0 + b1*xg)\n",
    "plt.xlabel(\"log(avg weekday entries)\"); plt.ylabel(\"log(betweenness)\")\n",
    "plt.title(\"WMATA: centrality vs ridership (log–log)\")\n",
    "plt.tight_layout(); plt.savefig(\"figs/centrality_vs_ridership.png\", dpi=200); plt.close()\n",
    "\n",
    "# Horizontal bar chart of “punch above weight”\n",
    "plt.figure(figsize=(8,5))\n",
    "(top_hidden.assign(name=lambda d: d[\"station_name\"].str.slice(0,24))\n",
    "           .sort_values(\"residual\")\n",
    "           .plot(kind=\"barh\", x=\"name\", y=\"residual\", legend=False,\n",
    "                 title=\"Stations that punch above their weight\"))\n",
    "plt.xlabel(\"Positive residual (more central than entries predict)\")\n",
    "plt.tight_layout(); plt.savefig(\"figs/punch_above_weight.png\", dpi=200); plt.close()\n",
    "\n",
    "print(\"✅ Saved: top_hidden_chokepoints.csv, top_crowded_not_central.csv,\",\n",
    "      \"figs/centrality_vs_ridership.png, figs/punch_above_weight.png\")\n",
    "top_hidden.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7688c772-442c-42e9-87c5-efbb6716a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> wmata_ridership_by_station_2024_daily.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rid = pd.read_csv(\"wmata_ridership_by_station_2024.csv\")\n",
    "# if numbers look huge, treat them as year totals and divide by weekday count\n",
    "weekday_days_2024 = pd.date_range(\"2024-01-01\", \"2024-12-31\", freq=\"B\").size  # Mon–Fri ≈ 262 in 2024\n",
    "rid[\"avg_weekday_entries_2024\"] = rid[\"avg_weekday_entries_2024\"] / weekday_days_2024\n",
    "rid.to_csv(\"wmata_ridership_by_station_2024_daily.csv\", index=False)\n",
    "print(\"Saved -> wmata_ridership_by_station_2024_daily.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36b61941-2ccf-41e8-96fa-3b3c5607f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here: C:\\Users\\Arsh\\Downloads\n",
      "PNGs: ['figs\\\\centrality_vs_ridership.png', 'figs\\\\damage_rate_dca_bwi.png', 'figs\\\\monthly_trend_dca_bwi.png', 'figs\\\\monthly_trend_rolling.png', 'figs\\\\punch_above_weight.png', 'figs\\\\top_species_damaging_dca_bwi.png']\n",
      "CSVs: ['apple_quality.csv', 'Arsh-IAM-Admin_credentials (1).csv', 'Arsh-IAM-Admin_credentials.csv', 'arsh.malik_accessKeys.csv', 'arsh.malik_credentials.csv', 'CrashData_test_6478750435646127290.csv', 'customers.csv', 'Data Range (1).csv', 'Data Range.csv', 'diabetes.csv', 'insurance.csv', 'line_items.csv', 'MillionSongsFinal.csv', 'orders.csv', 'Primary.csv', 'products.csv', 'results.csv', 'results2.csv', 'ridership_export.csv', 'Social_Network_Ads.csv', 'top_crowded_not_central.csv', 'top_hidden_chokepoints.csv', 'Tweets.csv', 'wmata_betweenness.csv', 'wmata_ridership_by_station_2024.csv', 'wmata_ridership_by_station_2024_daily.csv']\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "print(\"Here:\", os.getcwd())\n",
    "print(\"PNGs:\", glob.glob(\"figs/*.png\"))\n",
    "print(\"CSVs:\", glob.glob(\"*.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bda6649-39e7-4bff-a0ef-a864aba90310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> figs/crowded_not_central.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "top_crowded = (pd.read_csv(\"top_crowded_not_central.csv\")\n",
    "                 .sort_values(\"residual\", ascending=True)\n",
    "                 .head(10))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(top_crowded[\"station_name\"][::-1], (-top_crowded[\"residual\"])[::-1])\n",
    "plt.xlabel(\"Magnitude of negative residual\")\n",
    "plt.title(\"Crowded but not central\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/crowded_not_central.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved -> figs/crowded_not_central.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a91bd6-b3e3-4883-8923-05fb8bbaf032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
